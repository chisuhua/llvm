; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=ppu -mattr=+v -verify-machineinstrs < %s \
; RUN:   | FileCheck %s -check-prefix=RV32IV

declare i32 @llvm.ppu.setvl(i32)
declare <vscale x 1 x i32> @llvm.ppu.vadd(<vscale x 1 x i32>, <vscale x 1 x i32>, i32)
declare <vscale x 1 x i32> @llvm.ppu.vsub(<vscale x 1 x i32>, <vscale x 1 x i32>, i32)
declare <vscale x 1 x i32> @llvm.ppu.vmul(<vscale x 1 x i32>, <vscale x 1 x i32>, i32)
declare <vscale x 1 x i32> @llvm.ppu.vand(<vscale x 1 x i32>, <vscale x 1 x i32>, i32)
declare <vscale x 1 x i32> @llvm.ppu.vor(<vscale x 1 x i32>, <vscale x 1 x i32>, i32)
declare <vscale x 1 x i32> @llvm.ppu.vxor(<vscale x 1 x i32>, <vscale x 1 x i32>, i32)
declare <vscale x 1 x i32> @llvm.ppu.vlw(i32*, i32)
declare void @llvm.ppu.vsw(i32*, <vscale x 1 x i32>, i32)

; R[0..n] = A[0..n] + B[0..n]
define void @foo(i32 %n.0, i32* %R.0, i32* %A.0, i32* %B.0) {
; RV32IV-LABEL: foo:
; RV32IV:       # %bb.0: # %entry
; RV32IV-NEXT:    vconfig 96
; RV32IV-NEXT:  .LBB0_1: # %loop
; RV32IV-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32IV-NEXT:    vsetvl a4, a0
; RV32IV-NEXT:    vlw v0, 0(a3)
; RV32IV-NEXT:    vlw v1, 0(a2)
; RV32IV-NEXT:    vadd v1, v1, v0
; RV32IV-NEXT:    vsub v1, v1, v0
; RV32IV-NEXT:    vmul v1, v1, v0
; RV32IV-NEXT:    vand v1, v1, v0
; RV32IV-NEXT:    vor v1, v1, v0
; RV32IV-NEXT:    vxor v0, v1, v0
; RV32IV-NEXT:    vsw v0, 0(a1)
; RV32IV-NEXT:    slli a5, a4, 2
; RV32IV-NEXT:    add a1, a1, a5
; RV32IV-NEXT:    add a3, a3, a5
; RV32IV-NEXT:    add a2, a2, a5
; RV32IV-NEXT:    sub a0, a0, a4
; RV32IV-NEXT:    bnez a4, .LBB0_1
; RV32IV-NEXT:  # %bb.2: # %exit
; RV32IV-NEXT:    vconfig 1
; RV32IV-NEXT:	.cfi_def_cfa_offset 0
; RV32IV-NEXT:    ret
entry:
	br label %loop
loop:
	%n = phi i32 [%n.0, %entry], [%n.rem, %loop]
	%A = phi i32* [%A.0, %entry], [%A.rem, %loop]
	%B = phi i32* [%B.0, %entry], [%B.rem, %loop]
	%R = phi i32* [%R.0, %entry], [%R.rem, %loop]
	%vl = call i32 @llvm.ppu.setvl(i32 %n)
	%v.A = call <vscale x 1 x i32> @llvm.ppu.vlw(i32* %A, i32 %vl)
	%v.B = call <vscale x 1 x i32> @llvm.ppu.vlw(i32* %B, i32 %vl)

	%v.R1 = call <vscale x 1 x i32> @llvm.ppu.vadd(<vscale x 1 x i32> %v.A, <vscale x 1 x i32> %v.B, i32 %vl)
	%v.R2 = call <vscale x 1 x i32> @llvm.ppu.vsub(<vscale x 1 x i32> %v.R1, <vscale x 1 x i32> %v.B, i32 %vl)
	%v.R3 = call <vscale x 1 x i32> @llvm.ppu.vmul(<vscale x 1 x i32> %v.R2, <vscale x 1 x i32> %v.B, i32 %vl)
	%v.R4 = call <vscale x 1 x i32> @llvm.ppu.vand(<vscale x 1 x i32> %v.R3, <vscale x 1 x i32> %v.B, i32 %vl)
	%v.R5 = call <vscale x 1 x i32> @llvm.ppu.vor(<vscale x 1 x i32> %v.R4, <vscale x 1 x i32> %v.B, i32 %vl)
	%v.R6 = call <vscale x 1 x i32> @llvm.ppu.vxor(<vscale x 1 x i32> %v.R5, <vscale x 1 x i32> %v.B, i32 %vl)

	call void @llvm.ppu.vsw(i32* %R, <vscale x 1 x i32> %v.R6, i32 %vl)
	%n.rem = sub i32 %n, %vl
	%A.rem = getelementptr i32, i32* %A, i32 %vl
	%B.rem = getelementptr i32, i32* %B, i32 %vl
	%R.rem = getelementptr i32, i32* %R, i32 %vl
	%again = icmp ne i32 %vl, 0
	br i1 %again, label %loop, label %exit
exit:
	ret void
}
